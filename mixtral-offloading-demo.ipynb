{"cells":[{"cell_type":"markdown","source":["利用約12G GPU RAM (T4 GPU 16G) 即可offloading mistral 8x7b model"],"metadata":{"id":"utGoDYL3apyX"}},{"cell_type":"markdown","metadata":{"id":"OW1moHJ1TdhO"},"source":["# Mixtral in Colab\n","\n","Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n","\n","To learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub."]},{"cell_type":"markdown","metadata":{"id":"2-dvAX_hKZT4"},"source":["One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n","\n","\n","<details>\n","\n","<summary>How to balance between RAM and GPU VRAM usage</summary>\n","\n","You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n","\n","Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"Y8MhvkC7TKEL"},"source":["## Install and import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7qY7ebqX7T7"},"outputs":[],"source":["# fix numpy in colab\n","import numpy\n","from IPython.display import clear_output\n","\n","# fix triton in colab\n","!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n","!cd mixtral-offloading && pip install -q -r requirements.txt\n","!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n","\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgpjnV7fV49W"},"outputs":[],"source":["import sys\n","\n","sys.path.append(\"mixtral-offloading\")\n","import torch\n","from torch.nn import functional as F\n","from hqq.core.quantize import BaseQuantizeConfig\n","from huggingface_hub import snapshot_download\n","from IPython.display import clear_output\n","from tqdm.auto import trange\n","from transformers import AutoConfig, AutoTokenizer\n","from transformers.utils import logging as hf_logging\n","\n","from src.build_model import OffloadConfig, QuantConfig, build_model"]},{"cell_type":"markdown","metadata":{"id":"OkSYibHcTQsH"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mIpePTMFyRY"},"outputs":[],"source":["model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","\n","config = AutoConfig.from_pretrained(quantized_model_name)\n","\n","device = torch.device(\"cuda:0\")\n","\n","##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n","offload_per_layer = 4\n","# offload_per_layer = 5\n","###############################################################\n","\n","num_experts = config.num_local_experts\n","\n","offload_config = OffloadConfig(\n","    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n","    offload_size=config.num_hidden_layers * offload_per_layer,\n","    buffer_size=4,\n","    offload_per_layer=offload_per_layer,\n",")\n","\n","\n","attn_config = BaseQuantizeConfig(\n","    nbits=4,\n","    group_size=64,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n","\n","\n","ffn_config = BaseQuantizeConfig(\n","    nbits=2,\n","    group_size=16,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n","\n","\n","model = build_model(\n","    device=device,\n","    quant_config=quant_config,\n","    offload_config=offload_config,\n","    state_path=state_path,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Z4hBFYtPTUzT"},"source":["## Run the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zf4GkspecSm8"},"outputs":[],"source":["from transformers import TextStreamer\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","past_key_values = None\n","sequence = None\n","\n","seq_len = 0\n","while True:\n","  print(\"User: \", end=\"\")\n","  user_input = input()\n","  print(\"\\n\")\n","\n","  user_entry = dict(role=\"user\", content=user_input)\n","  input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n","\n","  if past_key_values is None:\n","    attention_mask = torch.ones_like(input_ids)\n","  else:\n","    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n","    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n","\n","  print(\"Mixtral: \", end=\"\")\n","  result = model.generate(\n","    input_ids=input_ids,\n","    attention_mask=attention_mask,\n","    past_key_values=past_key_values,\n","    streamer=streamer,\n","    do_sample=True,\n","    temperature=0.9,\n","    top_p=0.9,\n","    max_new_tokens=512,\n","    pad_token_id=tokenizer.eos_token_id,\n","    return_dict_in_generate=True,\n","    output_hidden_states=True,\n","  )\n","  print(\"\\n\")\n","\n","  sequence = result[\"sequences\"]\n","  past_key_values = result[\"past_key_values\"]"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}