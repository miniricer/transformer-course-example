{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMKetqmWCfKfDa3cpBHEvFf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["在這個範例，我們將要\n","\n","*   step1 轉換模型成爲 8-bit 量化\n","*   step2 跑 llama.cpp， 並建立起來\n","*   step3 跑 infer  "],"metadata":{"id":"izmi79jq6b42"}},{"cell_type":"markdown","source":["介紹llama.cpp [英文](https://github.com/ggerganov/llama.cpp)， [中文](https://ithelp.ithome.com.tw/m/articles/10331129)"],"metadata":{"id":"AE6n8GzbO7f4"}},{"cell_type":"markdown","source":["HuggingFace 上的[量化模型](https://huggingface.co/TheBloke)"],"metadata":{"id":"ZH7Mp7dRPv5Y"}},{"cell_type":"markdown","source":["[參考](https://colab.research.google.com/drive/1QzFsWru1YLnTVK77itWEASCPnIH7IDPo#scrollTo=sbY-LpGoSHbi)\n","\n","\n","[mistral-7b-v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n","\n","[llama.cpp server](https://github.com/ggerganov/llama.cpp/tree/master/examples/server)\n","\n","要在編輯--> 筆記本設定 -->指定T4"],"metadata":{"id":"JNHHM2kcheLx"}},{"cell_type":"markdown","source":["#step 1 轉換模型成爲 8-bit 量化"],"metadata":{"id":"gMKWOLKfRVGk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vn1RSxBefVmh"},"outputs":[],"source":["!git clone https://github.com/ggerganov/llama.cpp.git"]},{"cell_type":"code","source":["%cd llama.cpp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGfzFUjOfY38","executionInfo":{"status":"ok","timestamp":1704871345162,"user_tz":-480,"elapsed":287,"user":{"displayName":"daucheng lyu","userId":"10574303180162034414"}},"outputId":"1b358fd7-1550-4321-91d0-9bfb5fdc8d89"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llama.cpp\n"]}]},{"cell_type":"markdown","source":["安裝package， 安裝完之後，從新啓動session"],"metadata":{"id":"70Cbp4hWN375"}},{"cell_type":"code","source":["pip install -r requirements.txt"],"metadata":{"id":"QrWSOtxumaTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["下載模型，做convert 成爲 8-bit\n","下載時間約10分鐘"],"metadata":{"id":"zqIrHp4aNrqR"}},{"cell_type":"code","source":["from huggingface_hub import snapshot_download\n","model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n","snapshot_download(repo_id=model_id, local_dir=\"mistral-7b\",\n","                  local_dir_use_symlinks=False, revision=\"main\")"],"metadata":{"id":"sA3rk2QtM35o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["模型轉換, 大約花20分鐘\n","\n","這裏介紹Mistral 8x7B [模型](https://huggingface.co/mistralai) [論文](https://arxiv.org/pdf/2401.04088.pdf) [blog](https://mistral.ai/news/mixtral-of-experts/)\n","\n","MOE [Mixture of Expert](https://huggingface.co/blog/moe)"],"metadata":{"id":"MdrkQYQRNz7q"}},{"cell_type":"code","source":["!python convert.py mistral-7b --outfile mistral-7b.gguf --outtype q8_0"],"metadata":{"id":"uqzqhOrCpnDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["檢查模型是否存在"],"metadata":{"id":"zhki7TLdOb4Q"}},{"cell_type":"code","source":["!ls -la mistral-7b.gguf\n","#size 7.7GB"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2fdk67QvCh_","executionInfo":{"status":"ok","timestamp":1704872626759,"user_tz":-480,"elapsed":6,"user":{"displayName":"daucheng lyu","userId":"10574303180162034414"}},"outputId":"aab396bb-80b1-4358-d157-5c2481ae67b4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r-- 1 root root 7695857824 Jan 10 07:31 mistral-7b.gguf\n"]}]},{"cell_type":"markdown","source":["# step2 跑 llama.cpp， 並建立起來"],"metadata":{"id":"Jk_RdcnNRidH"}},{"cell_type":"code","source":["#有GPU的時候\n","!make LLAMA_CUBLAS=1"],"metadata":{"id":"StDTrUQ-foqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./main -h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v93bNrM8i-B1","executionInfo":{"status":"ok","timestamp":1704872636387,"user_tz":-480,"elapsed":1061,"user":{"displayName":"daucheng lyu","userId":"10574303180162034414"}},"outputId":"9c8b5cef-73e1-40d0-9522-99dc643ce630"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","usage: ./main [options]\n","\n","options:\n","  -h, --help            show this help message and exit\n","      --version         show version and build info\n","  -i, --interactive     run in interactive mode\n","  --interactive-first   run in interactive mode and wait for input right away\n","  -ins, --instruct      run in instruction mode (use with Alpaca models)\n","  -cml, --chatml        run in chatml mode (use with ChatML-compatible models)\n","  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n","  -r PROMPT, --reverse-prompt PROMPT\n","                        halt generation at PROMPT, return control in interactive mode\n","                        (can be specified more than once for multiple prompts).\n","  --color               colorise output to distinguish prompt and user input from generations\n","  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n","  -t N, --threads N     number of threads to use during generation (default: 2)\n","  -tb N, --threads-batch N\n","                        number of threads to use during batch and prompt processing (default: same as --threads)\n","  -p PROMPT, --prompt PROMPT\n","                        prompt to start generation with (default: empty)\n","  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n","  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n","  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n","                        not supported with --interactive or other interactive options\n","  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n","  --random-prompt       start with a randomized prompt.\n","  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n","  --in-prefix STRING    string to prefix user inputs with (default: empty)\n","  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n","  -f FNAME, --file FNAME\n","                        prompt file to start generation.\n","  -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)\n","  -c N, --ctx-size N    size of the prompt context (default: 512, 0 = loaded from model)\n","  -b N, --batch-size N  batch size for prompt processing (default: 512)\n","  --samplers            samplers that will be used for generation in the order, separated by ';', for example: \"top_k;tfs;typical;top_p;min_p;temp\"\n","  --sampling-seq        simplified sequence for samplers that will be used (default: kfypmt)\n","  --top-k N             top-k sampling (default: 40, 0 = disabled)\n","  --top-p N             top-p sampling (default: 0.9, 1.0 = disabled)\n","  --min-p N             min-p sampling (default: 0.1, 0.0 = disabled)\n","  --tfs N               tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n","  --typical N           locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n","  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)\n","  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)\n","  --presence-penalty N  repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n","  --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n","  --mirostat N          use Mirostat sampling.\n","                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n","                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n","  --mirostat-lr N       Mirostat learning rate, parameter eta (default: 0.1)\n","  --mirostat-ent N      Mirostat target entropy, parameter tau (default: 5.0)\n","  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n","                        modifies the likelihood of token appearing in the completion,\n","                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n","                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n","  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n","  --grammar-file FNAME  file to read grammar from\n","  --cfg-negative-prompt PROMPT\n","                        negative prompt to use for guidance. (default: empty)\n","  --cfg-negative-prompt-file FNAME\n","                        negative prompt file to use for guidance. (default: empty)\n","  --cfg-scale N         strength of guidance (default: 1.000000, 1.0 = disable)\n","  --rope-scaling {none,linear,yarn}\n","                        RoPE frequency scaling method, defaults to linear unless specified by the model\n","  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n","  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n","  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n","  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n","  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n","  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n","  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: 1.0)\n","  --yarn-beta-fast N    YaRN: low correction dim or beta (default: 32.0)\n","  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n","  --no-penalize-nl      do not penalize newline token\n","  --temp N              temperature (default: 0.8)\n","  --logits-all          return logits for all tokens in the batch (default: disabled)\n","  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n","  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: 400)\n","  --keep N              number of tokens to keep from the initial prompt (default: 0, -1 = all)\n","  --draft N             number of tokens to draft for speculative decoding (default: 8)\n","  --chunks N            max number of chunks to process (default: -1, -1 = all)\n","  -np N, --parallel N   number of parallel sequences to decode (default: 1)\n","  -ns N, --sequences N  number of sequences to decode (default: 1)\n","  -pa N, --p-accept N   speculative decoding accept probability (default: 0.5)\n","  -ps N, --p-split N    speculative decoding split probability (default: 0.1)\n","  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n","  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n","  --image IMAGE_FILE    path to an image file. use with multimodal models\n","  --mlock               force system to keep model in RAM rather than swapping or compressing\n","  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n","  --numa                attempt optimizations that help on some NUMA systems\n","                        if run without this previously, it is recommended to drop the system page cache before using this\n","                        see https://github.com/ggerganov/llama.cpp/issues/1437\n","  -ngl N, --n-gpu-layers N\n","                        number of layers to store in VRAM\n","  -ngld N, --n-gpu-layers-draft N\n","                        number of layers to store in VRAM for the draft model\n","  -ts SPLIT --tensor-split SPLIT\n","                        how to split tensors across multiple GPUs, comma-separated list of proportions, e.g. 3,1\n","  -mg i, --main-gpu i   the GPU to use for scratch and small tensors\n","  -nommq, --no-mul-mat-q\n","                        use cuBLAS instead of custom mul_mat_q CUDA kernels.\n","                        Not recommended since this is both slower and uses more VRAM.\n","  -gan N, --grp-attn-n N\n","                        group-attention factor (default: 1)\n","  -gaw N, --grp-attn-w N\n","                        group-attention width (default: 512.0)\n","  --verbose-prompt      print prompt before generation\n","  -dkvc, --dump-kv-cache\n","                        verbose print of the KV cache\n","  -nkvo, --no-kv-offload\n","                        disable KV offload\n","  -ctk TYPE, --cache-type-k TYPE\n","                        KV cache data type for K (default: f16)\n","  -ctv TYPE, --cache-type-v TYPE\n","                        KV cache data type for V (default: f16)\n","  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n","  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n","  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n","  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n","  -m FNAME, --model FNAME\n","                        model path (default: models/7B/ggml-model-f16.gguf)\n","  -md FNAME, --model-draft FNAME\n","                        draft model for speculative decoding\n","  -ld LOGDIR, --logdir LOGDIR\n","                        path under which to save YAML logs (no logging if unset)\n","  --override-kv KEY=TYPE:VALUE\n","                        advanced option to override model metadata by key. may be specified multiple times.\n","                        types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n","\n","log options:\n","  --log-test            Run simple logging test\n","  --log-disable         Disable trace logs\n","  --log-enable          Enable trace logs\n","  --log-file            Specify a log filename (without extension)\n","  --log-new             Create a separate new log file on start. Each log file will have unique name: \"<name>.<ID>.log\"\n","  --log-append          Don't truncate the old log file.\n"]}]},{"cell_type":"markdown","source":["# step3 跑 infer"],"metadata":{"id":"rTCLMnX1RsF8"}},{"cell_type":"code","source":["!./main -m mistral-7b.gguf -n 100 -p 'what is life in one sentence in US?' -ngl 8"],"metadata":{"id":"pUYxjvWjiJ3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./main -m mistral-7b.gguf -n 100 -p '中華電信的產品有哪些?' -ngl 8"],"metadata":{"id":"CbjImiwVVrQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./main -m mistral-7b.gguf -n 256 -p '用繁體中文介紹中華電信的歷史' -ngl 8"],"metadata":{"id":"OzSaaxKBV9zb"},"execution_count":null,"outputs":[]}]}