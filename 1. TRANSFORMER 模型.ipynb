{"cells":[{"cell_type":"markdown","metadata":{"id":"cMT8kRhypckK"},"source":["# Transformers能做什麼？\n","\n","[參考](https://huggingface.co/learn/nlp-course/zh-TW/chapter1/3?fw=pt)"]},{"cell_type":"markdown","metadata":{"id":"4P23WrSypckO"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"markdown","source":["Transformers 庫中最基本的對象是 pipeline() 函數。它將模型與其必要的預處理和後處理步驟連接起來，使我們能夠通過直接輸入任何文本並獲得最終的答案：!"],"metadata":{"id":"M1jrnnnxuGTL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9okPIolWpckQ"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\"I've been waiting for a HuggingFace course my whole life.\")"]},{"cell_type":"markdown","source":["我們也可以多傳幾句！"],"metadata":{"id":"1sKZtKYOuLPI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBETRmmhpckR","outputId":"2beea0be-552b-4b9e-be16-cdbc1412f2ef","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704942083332,"user_tz":-480,"elapsed":326,"user":{"displayName":"daucheng lyu","userId":"10574303180162034414"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":3}],"source":["classifier(\n","    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",")"]},{"cell_type":"markdown","source":["默認情況下，此pipeline選擇一個特定的預訓練模型，該模型已針對英語情感分析進行了微調。創建分類器對象時，將下載並緩存模型。如果您重新運行該命令，則將使用緩存的模型，無需再次下載模型。\n","\n","將一些文本傳遞到pipeline時涉及三個主要步驟：\n","\n","文本被預處理為\n","\n","1.   模型可以理解的格式。\n","2.   預處理的輸入被傳遞給模型。\n","3.   模型處理後輸出最終人類可以理解的結果。\n","\n","目前可用的一些pipeline是：\n","\n","特徵提取（獲取文本的向量表示）\n","\n","*   填充空缺\n","*   ner（命名實體識別）\n","*   問答\n","*   情感分析\n","*   文本摘要\n","*   文本生成\n","*   翻譯\n","*   零樣本分類\n","\n","讓我們來看看其中的一些吧！\n","\n","**零樣本分類**\n","我們將首先處理一項非常具挑戰性的任務，我們需要對尚未標記的文本進行分類。這是實際項目中的常見場景，因為注釋文本通常很耗時並且需要領域專業知識。對於這項任務**zero-shot-classification**pipeline非常強大：它允許您直接指定用於分類的標籤，因此您不必依賴預訓練模型的標籤。下面的模型展示瞭如何使用這兩個標籤將句子分類為正面或負面——但也可以使用您喜歡的任何其他標籤集對文本進行分類。"],"metadata":{"id":"z2QXbWiMuRq5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kd250XE3pckR"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"zero-shot-classification\")\n","classifier(\n","    \"This is a course about the Transformers library\",\n","    candidate_labels=[\"education\", \"politics\", \"business\"],\n",")"]},{"cell_type":"code","source":["!ls sample_data/"],"metadata":{"id":"oR5V1HwMeapP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["此pipeline稱為zero-shot，因為您不需要對數據上的模型進行微調即可使用它。它可以直接返回您想要的任何標籤列表的概率分數！\n","\n","**文本生成**\n","\n","現在讓我們看看如何使用pipeline來生成一些文本。這裡的主要使用方法是您提供一個提示，模型將通過生成剩餘的文本來自動完成整段話。這類似於許多手機上的預測文本功能。文本生成涉及隨機性，因此如果您沒有得到相同的如下所示的結果，這是正常的。"],"metadata":{"id":"ZfuRHaVyxdYv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YVKhfKspckS"},"outputs":[],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\")\n","generator(\"In this course, we will teach you how to\")"]},{"cell_type":"markdown","source":["您可以使用參數 num_return_sequences 控制生成多少個不同的序列，並使用參數 max_length 控制輸出文本的總長度。"],"metadata":{"id":"5vvATcXdyJVn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oQ1JjF9pckS"},"outputs":[],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n","generator(\n","    \"In this course, we will teach you how to\",\n","    max_length=30,\n","    num_return_sequences=2,\n",")"]},{"cell_type":"markdown","source":["**在pipeline中使用 Hub 中的其他模型**\n","\n","前面的示例使用了默認模型，但您也可以從 Hub 中選擇特定模型以在特定任務的pipeline中使用 - 例如，文本生成。轉到[模型中心（hub）](https://huggingface.co/models)並單擊左側的相應標籤將會只顯示該任務支持的模型。[例如這樣](https://huggingface.co/models?pipeline_tag=text-generation)。\n","讓我們試試 distilgpt2 模型吧！以下是如何在與以前相同的pipeline中加載它："],"metadata":{"id":"Jmw8agJ6x6MH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmsb6C_9pckS"},"outputs":[],"source":["from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\")\n","unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02eb21PipckT"},"outputs":[],"source":["from transformers import pipeline\n","\n","ner = pipeline(\"ner\", grouped_entities=True)\n","ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD55T3a7pckT"},"outputs":[],"source":["from transformers import pipeline\n","\n","question_answerer = pipeline(\"question-answering\")\n","question_answerer(\n","    question=\"Where do I work?\",\n","    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",")"]},{"cell_type":"markdown","source":["**文本摘要**\n","\n","文本摘要是將文本縮減為較短文本的任務，同時保留文本中的主要（重要）信息。下面是一個例子："],"metadata":{"id":"fU4I1FrkyvxP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QXM88AYpckU"},"outputs":[],"source":["from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\")\n","summarizer(\n","    \"\"\"\n","    America has changed dramatically during recent years. Not only has the number of\n","    graduates in traditional engineering disciplines such as mechanical, civil,\n","    electrical, chemical, and aeronautical engineering declined, but in most of\n","    the premier American universities engineering curricula now concentrate on\n","    and encourage largely the study of engineering science. As a result, there\n","    are declining offerings in engineering subjects dealing with infrastructure,\n","    the environment, and related issues, and greater concentration on high\n","    technology subjects, largely supporting increasingly complex scientific\n","    developments. While the latter is important, it should not be at the expense\n","    of more traditional engineering.\n","\n","    Rapidly developing economies such as China and India, as well as other\n","    industrial countries in Europe and Asia, continue to encourage and advance\n","    the teaching of engineering. Both China and India, respectively, graduate\n","    six and eight times as many traditional engineers as does the United States.\n","    Other industrial countries at minimum maintain their output, while America\n","    suffers an increasingly serious decline in the number of engineering graduates\n","    and a lack of well-educated engineers.\n","\"\"\"\n",")"]},{"cell_type":"markdown","source":["以下是中文的情感分析\n","參考 ：https://ithelp.ithome.com.tw/articles/10337606"],"metadata":{"id":"sLkb5BHCwX-f"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n","\n","# 使用指定的預訓練模型初始化序列分類模型\n","model = AutoModelForSequenceClassification.from_pretrained(\"techthiyanes/chinese_sentiment\")\n","# 使用指定的預訓練模型初始化分詞器\n","tokenizer = AutoTokenizer.from_pretrained(\"techthiyanes/chinese_sentiment\")\n","# 使用pipeline創建一個情感分析器\n","review_classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, top_k=5)"],"metadata":{"id":"qnUWs_vbwbZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 1\n","review_classifier(\"環境乾淨且服務人員很親切，餐點口味很棒而且食物新鮮!\")"],"metadata":{"id":"DbgWQH1swb6G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 2\n","review_classifier(\"上餐速度太慢，食物普通。\")"],"metadata":{"id":"z8I07IxJw2DE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["中文NER範例\n","在huggingface models 尋找 NER, 限定中文\n","\n"],"metadata":{"id":"f1C_iWvfzsWj"}},{"cell_type":"code","source":["from transformers import (\n","  BertTokenizerFast,\n","  AutoModel,\n","  pipeline,\n","  AutoModelForTokenClassification,\n",")\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n","model = AutoModelForTokenClassification.from_pretrained('ckiplab/bert-base-chinese-ner')\n","ner_chinese_classifier = pipeline('ner', model=model, tokenizer=tokenizer)"],"metadata":{"id":"yfw3nEp_0Rl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ner_chinese_classifier(\"我在板橋的中華電信大樓，時間是2024一月12號\")"],"metadata":{"id":"Z0G9zG5a1DvU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bias and limitations**\n","\n","Ask a Question\n","Open In Colab\n","Open In Studio Lab\n","如果您打算在正式的項目中使用經過預訓練或經過微調的模型。請注意：雖然這些模型是很強大，但它們也有侷限性。其中最大的一個問題是，爲了對大量數據進行預訓練，研究人員通常會蒐集所有他們能找到的內容，中間可能夾帶一些意識形態或者價值觀的刻板印象。\n","\n","爲了快速解釋清楚這個問題，讓我們回到一個使用 BERT 模型的 pipeline 的例子："],"metadata":{"id":"zaDCz4Re4lZB"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n","result = unmasker(\"This man works as a [MASK].\")\n","print([r[\"token_str\"] for r in result])\n","\n","result = unmasker(\"This woman works as a [MASK].\")\n","print([r[\"token_str\"] for r in result])"],"metadata":{"id":"6Co8oGYs1Yog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["關於LLM 產生偏差内容的文章：\n","\n","*   [BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation](https://arxiv.org/pdf/2101.11718.pdf)\n","\n","\n","*   [BBQ: A Hand-Built Bias Benchmark for Question Answering](https://arxiv.org/pdf/2110.08193.pdf)\n","\n","\n","*   [The Woman Worked as a Babysitter: On Biases in Language Generation](https://arxiv.org/pdf/1909.01326.pdf)"],"metadata":{"id":"8CubETJAojM6"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/zh-CN/chapter1/section3.ipynb","timestamp":1703217926921}],"gpuType":"T4"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}